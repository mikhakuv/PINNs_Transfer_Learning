(это пока черновик, который нужно будет дописать, проставить цитирования, отредактировать и перестроить некоторые графики)
# Введение
## Постановка задачи
В данной работе изучается эффективность методов трансферного обучения в решении дифференциальных уравнений с помощью технологии PINNs.
В качестве дифференциального уравнения рассматривается нелинейное уравнение Шрёдингера:  
$$i q_{t} + q_{xx} +q\cdot |q|^2(1-\alpha|q|^2+\beta |q|^4)=0$$
с начальным условием в виде солитона:  
$$q(x,0) = \sqrt{\frac{\mu e^{\left(x - x_{0}\right) \sqrt{\mu}}}{\left(\frac{1}{2} e^{\left(x - x_{0}\right) \sqrt{\mu}} + 1\right)^{2} - \frac{\alpha_0 \mu}{3} e^{2 \left(x - x_{0}\right) \sqrt{\mu}}}} e^{i \left(k x + \theta_{0}\right)},$$
$$где\ \mu = 4(k^{2} - w)$$
И граничным условием:  
$$q(x_0,t)=q(x_1,t)=0$$
Аналитическое решение такой задачи при $\alpha=\alpha_0$ известно:  
$$q(x,t) = \sqrt{\frac{\mu e^{\left(x - 2 k t - x_{0}\right) \sqrt{\mu}}}{\left(\frac{1}{2} e^{\left(x - 2 k t - x_{0}\right) \sqrt{\mu}} + 1\right)^{2} - \frac{\alpha_0 \mu}{3} e^{2 \left(x - 2 k t - x_{0}\right) \sqrt{\mu}}}} e^{i \left(k x - \omega t + \theta_{0}\right)},$$
$$где\ \mu = 4(k^{2} - w)$$
Значения коэффициентов: $k=1,\ w=0.88,\ x_0=-30,\ \theta_0=0,\ \beta=0$. При этом значения $\alpha$ и $\alpha_0$ совпадают во всех опытах(если не оговорено обратное) и принимают значения из множества $\lbrace 0.2; 0.3; 0.4 \rbrace$.  
Также в данной работе рассматривается пример практического применения методов трансферного обучения для улучшения решений, полученных ранее с помощью технологии FBPINNs.  
## Обзор литературы
Технология PINNs была предложена в 2018 году[1] и по сути предлагала свести задачу решения дифференциального уравнения к задаче оптимизации, которую можно решить с помощью инструментов машинного обучения. С тех пор эта технология была использована для большого числа прикладных задач[2-4] и ещё большее число раз модифицирована [5-7]. И поскольку решение уравнения по сути сводилось к обучению нейросети, было лишь вопросом времени применение идей из классического машинного обучения к этой технологии. В частности идея трансферного обучения была применена к PINNs уже через год её возникновения и есть приличное количество работ на эту тему[7-9]. Уникальность текущей работы заключается именно в рассматриваемом уравнении(до сих пор рассматривались более простые случаи), а также в попутно решаемой задаче об улучшении уже полученного ранее решения.
## План исследования
Основная часть данной работы разделена на 3 части: перенос обучаемых параметров, перенос значений, улучшение ранее полученных решений. В первых двух частях рассматриваются два разных метода трансферного обучения и обсуждается их эффективность для рассматриваемого случая. В последней же части предложен метод улучшения уже готовых решений с помощью трансферного обучения.
## Используемые метрики
В качестве основной метрики используется: $$Rel_h = \frac{\sqrt{ \sum\limits_{i=1}^N (|q_{i}^{truth}|-|q_{i}^{pred}|)^2 }}{\sqrt{\sum\limits_{i=1}^N |q_{i}^{truth}|^2}}$$
Дополнительно используются точности удовлетворения законам сохранения (выяснить как вычисляется Lw_per_mean):  
$$Lw_1$$
$$Lw_2$$
Эти метрики являются основными при $\alpha \neq \alpha_0$, поскольку в этом случае точное аналитическое решение $q_{truth}$ неизвестно.
# Основная часть
## Перенос обучаемых праметров
Основной идеей метода PINNs является нахождение решения в виде нейросети. Именно поэтому финальным результатом работы этого метода являются параметры (веса и смещения) нейросети, делающие из неё функцию, удовлетворяющую уравнению насколько это возможно. Но процесс поиска таких параметров - обучение нейросети является самой ресурсозатратной частью метода: занимает от нескольких минут до часов. Логично предположить, что это время можно сократить, если начинать обучение PINN не на случайных весах, а на тех, что были получены при решении похожих задач. Для проверки данной гипотезы в случае нелинейного уравнения Шрёдингера было проведено несколько опытов на небольшой области $x \in [-60,40], t \in [0,20]$ (см. таблицы 1 и 2). Для каждого из значений $\alpha \in [0.2, 0.3, 0.4]$ была обучена PINN с первоначально случайными параметрами(basic learning), а также другая PINN с параметрами, которые были взяты от предыдущего опыта для $\alpha=0.2$(weights transfer). Каждый опыт был повторён 10 раз, а результаты $Rel_h$ усреднены.  

**таблица 1, basic learning**  
|             | alpha=0.2 |          | alpha=0.3 |          | alpha=0.4 |          |
|-------------|-----------|----------|-----------|----------|-----------|----------|
| test_number | Time      | Rel_h    | Time      | Rel_h    | Time      | Rel_h    |
| 1           | 2h 2min   | 4,41E-02 | 2h 2min   | 1,80E-02 | 2h 2min   | 3,93E-02 |
| 2           | 2h 2min   | 4,24E-02 | 2h 2min   | 2,24E-02 | 2h 2min   | 2,31E-02 |
| 3           | 2h 2min   | 3,66E-02 | 2h 2min   | 7,73E-03 | 2h 2min   | 2,19E-02 |
| 4           | 2h 2min   | 1,52E-02 | 2h 2min   | 1,98E-02 | 2h 2min   | 3,97E-02 |
| 5           | 2h 2min   | 7,06E-03 | 2h 2min   | 3,60E-02 | 2h 2min   | 2,98E-02 |
| 6           | 2h 2min   | 1,90E-02 | 2h 2min   | 2,12E-02 | 2h 2min   | 5,85E-03 |
| 7           | 2h 3min   | 3,95E-02 | 2h 3min   | 1,14E-02 | 2h 3min   | 2,45E-02 |
| 8           | 2h 3min   | 1,20E-02 | 2h 3min   | 2,65E-02 | 2h 3min   | 8,21E-03 |
| 9           | 2h 3min   | 2,19E-02 | 2h 3min   | 2,90E-02 | 2h 3min   | 3,51E-02 |
| 10          | 2h 2min   | 2,12E-02 | 2h 2min   | 1,01E-02 | 2h 2min   | 2,78E-02 |
| average     |           | 2,59E-02 |           | 2,02E-02 |           | 2,55E-02 |

<!-- код этой таблицы в латехе
\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
        ~ & alpha=0.2 & ~ & alpha=0.3 & ~ & alpha=0.4 & ~ \\ \hline
        test\_number & Time & Rel\_h & Time & Rel\_h & Time & Rel\_h \\ \hline
        1 & 2h 2min & 4,41E-02 & 2h 2min & 1,80E-02 & 2h 2min & 3,93E-02 \\ \hline
        2 & 2h 2min & 4,24E-02 & 2h 2min & 2,24E-02 & 2h 2min & 2,31E-02 \\ \hline
        3 & 2h 2min & 3,66E-02 & 2h 2min & 7,73E-03 & 2h 2min & 2,19E-02 \\ \hline
        4 & 2h 2min & 1,52E-02 & 2h 2min & 1,98E-02 & 2h 2min & 3,97E-02 \\ \hline
        5 & 2h 2min & 7,06E-03 & 2h 2min & 3,60E-02 & 2h 2min & 2,98E-02 \\ \hline
        6 & 2h 2min & 1,90E-02 & 2h 2min & 2,12E-02 & 2h 2min & 5,85E-03 \\ \hline
        7 & 2h 3min & 3,95E-02 & 2h 3min & 1,14E-02 & 2h 3min & 2,45E-02 \\ \hline
        8 & 2h 3min & 1,20E-02 & 2h 3min & 2,65E-02 & 2h 3min & 8,21E-03 \\ \hline
        9 & 2h 3min & 2,19E-02 & 2h 3min & 2,90E-02 & 2h 3min & 3,51E-02 \\ \hline
        10 & 2h 2min & 2,12E-02 & 2h 2min & 1,01E-02 & 2h 2min & 2,78E-02 \\ \hline
        average & ~ & 2,59E-02 & ~ & 2,02E-02 & ~ & 2,55E-02 \\ \hline
    \end{tabular}
\end{table}
-->  

**таблица 2, parameters transfer**  
| source alpha=0.2 | alpha=0.2 |          | alpha=0.3 |          | alpha=0.4 |          |
|------------------|-----------|----------|-----------|----------|-----------|----------|
| test_number      | Time      | Rel_h    | Time      | Rel_h    | Time      | Rel_h    |
| 1                | 2h 2min   | 1,22E-02 | 2h 2min   | 1,12E-02 | 2h 2min   | 1,00E-02 |
| 2                | 2h 2min   | 1,36E-02 | 2h 2min   | 1,28E-02 | 2h 2min   | 1,11E-02 |
| 3                | 2h 2min   | 1,47E-02 | 2h 2min   | 1,25E-02 | 2h 2min   | 1,13E-02 |
| 4                | 2h 2min   | 5,56E-03 | 2h 2min   | 5,13E-03 | 2h 2min   | 4,74E-03 |
| 5                | 2h 2min   | 2,61E-03 | 2h 2min   | 2,62E-03 | 2h 2min   | 2,41E-03 |
| 6                | 2h 2min   | 6,88E-03 | 2h 2min   | 6,53E-03 | 2h 2min   | 5,91E-03 |
| 7                | 2h 2min   | 1,53E-02 | 2h 2min   | 1,32E-02 | 2h 2min   | 1,22E-02 |
| 8                | 2h 2min   | 3,77E-03 | 2h 2min   | 3,70E-03 | 2h 2min   | 3,29E-03 |
| 9                | 2h 2min   | 8,86E-03 | 2h 2min   | 8,29E-03 | 2h 2min   | 7,31E-03 |
| 10               | 2h 2min   | 6,29E-03 | 2h 2min   | 5,98E-03 | 2h 2min   | 5,28E-03 |
| average          |           | 8,97E-03 |           | 8,20E-03 |           | 7,35E-03 |

<!--код в латехе
\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
        source alpha=0.2 & alpha=0.2 & ~ & alpha=0.3 & ~ & alpha=0.4 & ~ \\ \hline
        test\_number & Time & Rel\_h & Time & Rel\_h & Time & Rel\_h \\ \hline
        1 & 2h 2min & 1,22E-02 & 2h 2min & 1,12E-02 & 2h 2min & 1,00E-02 \\ \hline
        2 & 2h 2min & 1,36E-02 & 2h 2min & 1,28E-02 & 2h 2min & 1,11E-02 \\ \hline
        3 & 2h 2min & 1,47E-02 & 2h 2min & 1,25E-02 & 2h 2min & 1,13E-02 \\ \hline
        4 & 2h 2min & 5,56E-03 & 2h 2min & 5,13E-03 & 2h 2min & 4,74E-03 \\ \hline
        5 & 2h 2min & 2,61E-03 & 2h 2min & 2,62E-03 & 2h 2min & 2,41E-03 \\ \hline
        6 & 2h 2min & 6,88E-03 & 2h 2min & 6,53E-03 & 2h 2min & 5,91E-03 \\ \hline
        7 & 2h 2min & 1,53E-02 & 2h 2min & 1,32E-02 & 2h 2min & 1,22E-02 \\ \hline
        8 & 2h 2min & 3,77E-03 & 2h 2min & 3,70E-03 & 2h 2min & 3,29E-03 \\ \hline
        9 & 2h 2min & 8,86E-03 & 2h 2min & 8,29E-03 & 2h 2min & 7,31E-03 \\ \hline
        10 & 2h 2min & 6,29E-03 & 2h 2min & 5,98E-03 & 2h 2min & 5,28E-03 \\ \hline
        average & ~ & 8,97E-03 & ~ & 8,20E-03 & ~ & 7,35E-03 \\ \hline
    \end{tabular}
\end{table}
-->

Таким образом был определён усреднённый рост точности при применении подобного подхода и он составляет не менее **59.5%**. Причём видно, что пока разница в коэффициентах $\alpha$ сравнительно небольшая, увеличение точности от неё практически не зависит(см. график 1).  
<p align="center"><img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/chart_1.PNG"><br><caption>график 1</caption></p>  

Видно, точность при применении подобного подхода действительно возрастает и его результативность подвтерждается критерием Стьюдента(надо разобраться, подходит ли он с математической точки зрения и вычислить).  

## Перенос значений
Но в случае PINNs можно действовать и другим способом. Так как задача оптимизации осложнена не высокой размерностью данных или большим их количеством, а видом функции потерь (которая содержит частные производные), упрощение вида функции потерь приведёт к существенному 
упрощению задачи оптимизации. Поэтому вместо переноса параметров модели можно обучить новую модель на решении $q_{sim}(x,t)$, полученном ранее, причём процесс переноса не будет требовать много времени. Таким образом обучение нейросети будет состоять из двух этапов:  
1) Перенос значений: аппроксимация решения похожей задачи, функция потерь имеет вид: $$Loss = \frac{\sum\limits_{i=1}^N (|q_{sim}^i-q_{pred}^i|^2)}{N}$$
2) Обычная задача оптимизации для случая PINNs: Обучение уравнению, начальным и граничным условиям, функция потерь имеет вид: $$Loss = \frac{\sum\limits_{i=1}^N (|q_{0}^i-q_{pred}^i|^2)}{N} + \frac{\sum\limits_{i=1}^N (|Eq(q_{pred}^i)|^2)}{N}$$
Для проверки предложенной идеи были проведены опыты на той же области $x \in [-60,40], t \in [0,20]$: для каждого значения $\alpha \in [0.2, 0.3, 0.4]$ проводилось 10 опытов с переносом значений от соответствующих решений из таблицы 1 для случая $\alpha=0.2$. Полученные результаты занесены в таблицу 3.  

**таблица 3, values transfer**  
| source alpha=0.2 | alpha=0.2 |          | alpha=0.3 |          | alpha=0.4 |          |
|------------------|-----------|----------|-----------|----------|-----------|----------|
| test_number      | Time      | Rel_h    | Time      | Rel_h    | Time      | Rel_h    |
| 1                | 2h 7min   | 1,76E-02 | 2h 7min   | 7,21E-03 | 2h 7min   | 1,16E-02 |
| 2                | 2h 7min   | 8,32E-03 | 2h 7min   | 1,15E-02 | 2h 7min   | 1,92E-02 |
| 3                | 2h 7min   | 2,50E-02 | 2h 7min   | 2,01E-02 | 2h 7min   | 3,34E-02 |
| 4                | 2h 7min   | 2,03E-02 | 2h 7min   | 2,11E-02 | 2h 7min   | 3,11E-02 |
| 5                | 2h 7min   | 1,94E-02 | 2h 7min   | 1,29E-02 | 2h 7min   | 1,59E-02 |
| 6                | 2h 7min   | 2,34E-02 | 2h 6min   | 1,82E-02 | 2h 7min   | 2,63E-02 |
| 7                | 2h 7min   | 9,63E-03 | 2h 7min   | 1,19E-02 | 2h 7min   | 2,61E-02 |
| 8                | 2h 7min   | 1,04E-02 | 2h 7min   | 1,73E-02 | 2h 7min   | 2,09E-02 |
| 9                | 2h 7min   | 1,38E-02 | 2h 7min   | 1,31E-02 | 2h 7min   | 9,51E-03 |
| 10               | 2h 6min   | 9,70E-03 | 2h 7min   | 2,10E-02 | 2h 7min   | 6,63E-03 |
| average          |           | 1,58E-02 |           | 1,54E-02 |           | 2,01E-02 |

<!-- код таблицы в латехе
\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
        source alpha=0.2 & alpha=0.2 & ~ & alpha=0.3 & ~ & alpha=0.4 & ~ \\ \hline
        test\_number & Time & Rel\_h & Time & Rel\_h & Time & Rel\_h \\ \hline
        1 & 2h 7min & 1,76E-02 & 2h 7min & 7,21E-03 & 2h 7min & 1,16E-02 \\ \hline
        2 & 2h 7min & 8,32E-03 & 2h 7min & 1,15E-02 & 2h 7min & 1,92E-02 \\ \hline
        3 & 2h 7min & 2,50E-02 & 2h 7min & 2,01E-02 & 2h 7min & 3,34E-02 \\ \hline
        4 & 2h 7min & 2,03E-02 & 2h 7min & 2,11E-02 & 2h 7min & 3,11E-02 \\ \hline
        5 & 2h 7min & 1,94E-02 & 2h 7min & 1,29E-02 & 2h 7min & 1,59E-02 \\ \hline
        6 & 2h 7min & 2,34E-02 & 2h 6min & 1,82E-02 & 2h 7min & 2,63E-02 \\ \hline
        7 & 2h 7min & 9,63E-03 & 2h 7min & 1,19E-02 & 2h 7min & 2,61E-02 \\ \hline
        8 & 2h 7min & 1,04E-02 & 2h 7min & 1,73E-02 & 2h 7min & 2,09E-02 \\ \hline
        9 & 2h 7min & 1,38E-02 & 2h 7min & 1,31E-02 & 2h 7min & 9,51E-03 \\ \hline
        10 & 2h 6min & 9,70E-03 & 2h 7min & 2,10E-02 & 2h 7min & 6,63E-03 \\ \hline
        average & ~ & 1,58E-02 & ~ & 1,54E-02 & ~ & 2,01E-02 \\ \hline
    \end{tabular}
\end{table}
-->

По результатам экспериментов получилось, что весь процесс целиком занимает примерно на 5 минут(~4% времени обучения) больше времени, чем классическое обучение без переноса значений. При этом средняя точность выросла не менее чем на **21%**(см график 2).  
<p align="center"><img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/chart_2.PNG"><br><caption>график 2</caption></p>  

Тем не менее видно, что точки кривой values_transfer целиком лежат в погрешности, что может говорить о сомнительности роста точности при использовании данного метода (опять же, можно посмотреть на критерий Стьюдента). Поэтому при применении этого метода в следующей главе он будет немного доработан.  

## Улучшение ранее полученных решений
Рассмотрим пример практической задачи, в которой могут быть использованы изученные выше методы.  
В процессе исследования технологии FBPINNs [ссылка на соответствующую статью] было было получено большое количество данных, соответствующих решению рассматриваемого в данной статье нелинейного уравнения Шрёдингера на больших областях($x \in [-70,230], t \in [0,100]$). Получение таких данных с помощью обычного PINN не представляется возможным в силу размера области: солитон распыляется, затухает, теряет форму. Но и сами данные неидеальны: в самом начале решение испытывает сильные осцилляции(см график 3), характерные при использовании плотного разбиения на области в технологии FBPINNs. Такие осцилляции при появлении вблизи начального условия могут влиять на поведение решения при больших временах и увеличивают ошибку.
<p align="center"><img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/raw_1_amplitude.png"><br><caption>график 3</caption></p>  

Таким образом возникает задача улучшения решений, полученных ранее. При этом в данной ситуации нельзя просто доучить существующую PINN, ведь технология FBPINNs обучает большое число PINN одновременно, каждая из которых определена на своей малой области. Более того, использование такой же конструкции приведёт к той же проблеме - осцилляции полученного решения. Это приводит к идее использования упомянутых выше подходов трансферного обучения. На графике 4 изображено сравнение метрики $Rel_h$ для каждого из подходов, полученное для малых областей ранее.  
<p align="center"><img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/chart_3.PNG"><br><caption>график 4</caption></p>  

Видно, что перенос параметров гораздо эффективнее, но применение его для текущей задачи невозможно по той же причине, что и дообучение: FBPINN состоит из большого числа PINN и перенос параметров сработал бы только в том случае, если бы использовались такие же PINN с такой же топологией. Поэтому в данной работе будет использован метод переноса значений. Для увеличения его эффективности можно оптимизировать этот процесс, избавившись от областей с околонулевыми значениями, характерными для решаемой задачи. Это можно сделать, оставив только точки удовлетворяющие условию $|q(x,t)|>q_{min}$, где значение $q_{min}=const=5\cdot10^{-3}$. Благодаря этому задача оптимизации сильно упростится, но можно упростить ещё сильнее, использовав хорошо знакомую идею сегментации по времени. В итоге данные решения от FBPINN будут сосредоточены вблизи солитона и разделены на части(см график 5), на каждой из которых последовательно будет обучена нейросеть.  
<p align="center"><img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_3_decomposition.png"><br><caption>график 5</caption></p>  

Для исследования результативности предложенной идеи было проведено несколько опытов для разных начальных данных. Были дополнительно использованы метрики $Lw_1$ и $Lw_2$, которые показательны для опыта 3, в котором метрика $Rel_h$ неприменима. Результаты применения данного подхода можно увидеть в таблице 4.  

**таблица 4**  
| exp name      | alpha | alpha_0 | params | time    | Lw_1   | Lw_2   | Rel_h  |
|---------------|-------|---------|--------|---------|--------|--------|--------|
| fbpinn 1      | 0,3   | 0,3     | 456000 | 57h 32m | 0,9174 | 0,9956 | 0,0204 |
| enhancement 1 | 0,3   | 0,3     | 103510 | 9h 8min | 0,1306 | 0,1500 | 0,0126 |
| fbpinn 2      | 0,3   | 0,3     | 342000 | 38h 59m | 0,3872 | 0,4261 | 0,0272 |
| enhancement 2 | 0,3   | 0,3     | 103510 | 9h 2min | 0,1809 | 0,1969 | 0,0122 |
| fbpinn 3      | 0,3   | 0,35    | 342000 | 39h 41m | 0,4455 | 0,2393 | -      |
| enhancement 3 | 0,3   | 0,35    | 103510 | 9h 5min | 0,1274 | 0,1508 | -      |

<!--
\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|l|}
    \hline
        exp name  & alpha & alpha\_0 & params & time & Lw\_1 & Lw\_2 & Rel\_h \\ \hline
        fbpinn 1 & 0,3 & 0,3 & 456000 & 57h 32m & 0,9174 & 0,9956 & 0,0204 \\ \hline
        enhancement 1 & 0,3 & 0,3 & 103510 & 9h 8min & 0,1306 & 0,1500 & 0,0126 \\ \hline
        fbpinn 2 & 0,3 & 0,3 & 342000 & 38h 59m & 0,3872 & 0,4261 & 0,0272 \\ \hline
        enhancement 2 & 0,3 & 0,3 & 103510 & 9h 2min & 0,1809 & 0,1969 & 0,0122 \\ \hline
        fbpinn 3 & 0,3 & 0,35 & 342000 & 39h 41m & 0,4455 & 0,2393 & - \\ \hline
        enhancement 3 & 0,3 & 0,35 & 103510 & 9h 5min & 0,1274 & 0,1508 & - \\ \hline
    \end{tabular}
\end{table}
-->

При сравнении строк fbpinn и соответствующих им enhancement можно видеть, что применение предложенного подхода позволило значительно увеличить точность решения по всем используемым метрикам. При этом исчезли нежелательные артефакты (см график 6).  
<p align="center"><img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_3_amplitude.png"><br><caption>график 6</caption></p>  

Отдельно стоит отметить опыт при $\alpha \neq \alpha_0$. Как уже отмечалось ранее этот случай интересен тем, что для него неизвестно аналитическое решение, а численные методы пока не позволяют решать его с высокой точностью. Предложенный подход позволяет получить достаточно точное численное решение на больших промежутках времени и согласно ему, солитон в данном случае не затухает (см график 7). Это можно считать практическим результатом данной работы.  
<p align="center"><img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_5_amplitude.png"><br><caption>график 7</caption></p>  

<!-- старая версия
Рассмотрим пример работы данного подхода:  
1. Исходный результат:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/raw_1_fig.png">  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/raw_1_amplitude.png">  

Разбиение данных:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_3_decomposition.png">  

Улучшенный результат:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_3_fig.png">  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_3_amplitude.png">  

Rel_h уменьшилась на **38%**, ошибки на законах - более чем на **70%**  
Можно проделать те же самые действия и для полученных данных:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_3+_fig.png">  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_3+_amplitude.png">  

Rel_h почему-то увеличилась, но ошибки на законах дополнительно уменьшились более чем на **49%**  

2. Исходный результат:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/raw_2_fig.png">  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/raw_2_amplitude.png">  

Разбиение данных:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_4_decomposition.png">  

Улучшенный результат:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_4_fig.png">  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_4_amplitude.png">  

Rel_h уменьшилась на **55%**, ошибки на законах - более чем на **53%**  
Можно проделать те же самые действия и для полученных данных:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_4+_fig.png">  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_4+_amplitude.png">  

Rel_h почему-то увеличилась, но ошибки на законах дополнительно уменьшились более чем на **75%**  

Опыты для $\alpha \neq \alpha_0$
---
3. Исходный результат:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/raw_3_fig.png">  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/raw_3_amplitude.png">  

Разбиение данных:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_5_decomposition.png">  

Улучшенный результат:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_5_fig.png">  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_5_amplitude.png">  

Ошибки на законах уменьшились более чем на **36%**  
Можно проделать те же самые действия и для полученных данных:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_5+_fig.png">  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/transfer_learning/exp_5+_amplitude.png">  

Ошибки на законах уменьшатся дополнительно более чем на **39%**
-->

Более подробная статистика доступна в файлах: [performance_table.xlsx](https://github.com/mikhakuv/PINNs/blob/main/statistics/performance_table_transfer_learning.xlsx), [enhancement_stats.xlsx](https://github.com/mikhakuv/PINNs/blob/main/statistics/enhancement_stats.xlsx)

# Заключение
## Обсуждение результатов
## Ссылки на источники
[1] - короче Maziar Raissi, George Em Karniadakis Physical Informed Deep Learning  
[2] - [4] - примеры прикладного использования PINNs  
[5] - [7] - улучшения PINNs: FBPINNs, etc  
[7] - [9] - статьи на тему Transfer Learning in PINNs  
